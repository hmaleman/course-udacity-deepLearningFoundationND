{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evalutation and Testing\n",
    "- Course: Deep Learning Foundation Nanodegree\n",
    "- Section: Convolutional Neural Networks\n",
    "- Lesson 1: Model Evalutation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic: Testing\n",
    "Testing is used for Model evaluation, i.e. whether the generated model is good and generalizes well, or if it overfits the training set. Usually the test is a sub-set from the available Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sklearn v 0.18\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic: Confusion Matrix - Classification Model\n",
    "This matrix helps idenfity a **Classification Model** perfomance by counting the results in 4 bins:\n",
    "- True positive: Good prediction for the target class\n",
    "- True negative: Good prediction for the complementary class\n",
    "- False negative: Bad prediction for the complementary class\n",
    "- False positive: Bad prediction for the target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic: Accuracy - Classification Model\n",
    "Number of correct classified inputs, by the total number of inputs.\n",
    "\n",
    "    (True postive + True negative) / Total nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic: Metrics - Regression Model\n",
    "\n",
    "- MAE - Mean absolute error: Simple difference between prediction and input. This metric is not differentiable, hence can't be used for gradient descent.\n",
    "\n",
    "- MSE - Mean squared error: Square of the distance (difference) between prediction and input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "classifier = LinearRegression()\n",
    "classifier.fit(X,y)\n",
    "\n",
    "guesses = classifier.predict(X)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "MAE = mean_absolute_error(y, guesses)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE = mean_absolute_error(y, guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the MSE, it's possible to calculate the R2 score metric for the regression model. This is given by the formula\n",
    "\n",
    "    R2 = 1 - (MSE from model / MSE from average(inputs))\n",
    "\n",
    "- R2 -> 0: Bad model. Model is no better than a simple input average.\n",
    "- R2 -> 1: Good model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "metricR2 = r2_score(y, guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Topic: Types of Error\n",
    "Oversimplifying the problem - Underfitting. High bias, do not do well on the training set already and also bad on the training set.\n",
    "\n",
    "Overcomplicate a problem - Overfitting. High variance, do well on the training set but do not on the test set.\n",
    "\n",
    "Good model compromises both. Strategy? From Underfit toward Overfit and stop.\n",
    "\n",
    "However a model decision shall not be made based on the performance on the testing set, otherwise we risk bleeding the test set inside the model and not generalize it well. One option to it is not to split the model in only Traning and Testing, but split it into Training, Cross Validation (for model decision) and Test set (to be evaluated at the very end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic: K-Fold Cross Validation\n",
    "In orde to not loose data by statically spliting th trainin and test set.\n",
    "\n",
    "The K-Fold technique splits the entire set into K subsets. Use K-1 subsets to train the K models, each time varying the used training subsets, then average the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(datasetSize, subsetSize, shuffle=True) #shuffle data to avoid bias\n",
    "for trainIndex, testIndex in kf:\n",
    "    #train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
